{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grayscale\n",
    "\n",
    "> Grayscale Image Colorization with GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the environment by running the following to format the dataset and define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and format the dataset\n",
    "\n",
    "%rm -rf grayscale-dataset-data\n",
    "\n",
    "! if [ ! -d grayscale-dataset-data ] ; \\\n",
    "  then wget https://github.com/anthonytedja/grayscale-dataset/archive/refs/tags/data.zip; \\\n",
    "    unzip data.zip; \\\n",
    "    rm data.zip; \\\n",
    "fi\n",
    "\n",
    "!mkdir grayscale-dataset-data/test\n",
    "%cd grayscale-dataset-data/dataset\n",
    "!mv $(ls | tail -n 1000) ../test\n",
    "%cd ..\n",
    "\n",
    "!mv dataset train\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Run the below cell to train and test on a smaller subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls RESULT/grayscale-dataset-data/ | wc -l\n",
    "\n",
    "!mkdir grayscale-dataset-data/train_2\n",
    "!mkdir grayscale-dataset-data/test_2\n",
    "\n",
    "%cd grayscale-dataset-data/train/\n",
    "!mv $(ls | tail -n 5500) ../train_2\n",
    "%cd ../test\n",
    "!mv $(ls | tail -n 990) ../test_2\n",
    "%cd ../..\n",
    "\n",
    "!ls grayscale-dataset-data/test | wc -l\n",
    "!ls grayscale-dataset-data/test_2 | wc -l\n",
    "!ls grayscale-dataset-data/train | wc -l\n",
    "!ls grayscale-dataset-data/train_2 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Directories\n",
    "        self.ROOT_DIR = os.path.abspath('./')\n",
    "        self.DATASET = \"grayscale-dataset-data/\"\n",
    "\n",
    "        self.TRAIN_PATH = os.path.join(self.ROOT_DIR, self.DATASET, \"train/\")\n",
    "        self.TEST_PATH = os.path.join(self.ROOT_DIR, self.DATASET, \"test/\")\n",
    "\n",
    "        self.MODEL_DIR = 'models'\n",
    "        self.MODEL_C = self.MODEL_DIR + '/colorization.pt'\n",
    "        self.MODEL_D = self.MODEL_DIR + '/discriminator.pt'\n",
    "        self.OUTPUT_PATH = os.path.join(self.ROOT_DIR, 'results/'+self.DATASET)\n",
    "\n",
    "        # Data\n",
    "        self.IMAGE_SIZE = 224\n",
    "        self.BATCH_SIZE = 10\n",
    "\n",
    "        # Training\n",
    "        self.EPOCHS = 5\n",
    "\n",
    "        self.GRADIENT_PENALTY_WEIGHT = 10\n",
    "        self.CHECK_PER = 100\n",
    "        self.LR = 2e-5\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess(imgs):\n",
    "    imgs = imgs * 255\n",
    "    imgs[imgs > 255] = 255\n",
    "    imgs[imgs < 0] = 0\n",
    "    return imgs.astype(np.uint8)\n",
    "\n",
    "\n",
    "def reconstruct(batchX, predictedY):\n",
    "    result = np.concatenate((batchX, predictedY))\n",
    "    result = np.transpose(result, (1, 2, 0))\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_Lab2BGR)\n",
    "    return result\n",
    "\n",
    "\n",
    "def wasserstein_loss(inputs, real_or_fake):\n",
    "    return -torch.mean(inputs) if real_or_fake else torch.mean(inputs)\n",
    "\n",
    "\n",
    "def random_weighted_average(inputs):\n",
    "    weight = torch.rand((config.BATCH_SIZE, 1, 1, 1)).to(config.DEVICE)\n",
    "    return (weight * inputs[0]) + ((1 - weight) * inputs[1])\n",
    "\n",
    "\n",
    "def gradient_penalty_loss(y_pred, averaged_samples, gradient_penalty_weight):\n",
    "    gradients = torch.autograd.grad(y_pred, averaged_samples,\n",
    "                                    grad_outputs=torch.ones(y_pred.size(), device=config.DEVICE),\n",
    "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = torch.mean(((gradients + 1e-16).norm(2, dim=1) - 1) ** 2) * gradient_penalty_weight\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def partial_gp_loss(y_pred, averaged_samples, gradient_penalty_weight):\n",
    "    gradients = torch.autograd.grad(\n",
    "        y_pred, averaged_samples,\n",
    "        grad_outputs=torch.ones(y_pred.size(), device=config.DEVICE),\n",
    "        )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = (((gradients+1e-16).norm(2, dim=1) - 1) ** 2).mean() * gradient_penalty_weight\n",
    "    return gradient_penalty.mean()\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizeDataLoader(Dataset):\n",
    "    def __init__(self, color_path, img_size=224):\n",
    "\n",
    "        if not os.path.isdir(color_path):\n",
    "            raise Exception(\"The path is not a directory\")\n",
    "        if img_size != 224:\n",
    "            raise Exception(\"The image size must be 224\")\n",
    "\n",
    "        self.color_path = color_path\n",
    "        self.img_size = config.IMAGE_SIZE\n",
    "        self.color_channels = 3\n",
    "        self.gray_channels = 1\n",
    "        self.data_color = []\n",
    "        self.filelist = os.listdir(self.color_path)[:None]\n",
    "        self.size = len(self.filelist)\n",
    "\n",
    "        for path_images_name in glob.glob(self.color_path + '*'):\n",
    "            self.data_color.append(path_images_name)\n",
    "\n",
    "        if len(self.data_color) == 0:\n",
    "            print(self.color_path)\n",
    "            raise Exception(\"No images found in directory:\", self.color_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_color)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grey_img, color_img, original_images_shape = self.read_img(idx)\n",
    "        return self.transform(grey_img), self.transform(color_img), original_images_shape\n",
    "\n",
    "    def transform(self, img):\n",
    "        trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        return trans(img)\n",
    "\n",
    "    def read_img(self, idx):\n",
    "        img_color_path = self.data_color[idx]\n",
    "        img_color = cv2.imread(img_color_path)\n",
    "\n",
    "        lab_img = cv2.cvtColor(\n",
    "            cv2.resize(img_color, (self.img_size, self.img_size)),\n",
    "            cv2.COLOR_BGR2Lab)\n",
    "        original_shape = img_color.shape\n",
    "\n",
    "        return (\n",
    "            np.reshape(lab_img[:, :, 0], (self.img_size, self.img_size, 1)),\n",
    "            np.reshape(lab_img[:, :, 1:],(self.img_size, self.img_size,2)),\n",
    "            original_shape,\n",
    "            )\n",
    "\n",
    "def testing_colorize_dataloader():\n",
    "    color_loader = ColorizeDataLoader(config.TRAIN_PATH)\n",
    "    grey_img, color_img, original_images_shape = color_loader[0]\n",
    "\n",
    "    print('grey_img: ', grey_img.shape)\n",
    "    print('color_img: ', color_img.shape)\n",
    "    print('original_images_shape: ', original_images_shape)\n",
    "\n",
    "def checking_data_loader():\n",
    "    color_loader = ColorizeDataLoader(config.TRAIN_PATH)\n",
    "    test_dataloader = DataLoader(\n",
    "        color_loader, batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True, num_workers=2, drop_last=True)\n",
    "    for idx, (grey_img, color_img, _) in enumerate(tqdm(test_dataloader)):\n",
    "        continue\n",
    "    print('Validation passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlockDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.conv_2 = ConvBlockDiscriminator(64, 128, 4, 2, 1)\n",
    "        self.conv_3 = ConvBlockDiscriminator(128, 256, 4, 2, 1)\n",
    "        self.conv_4 = ConvBlockDiscriminator(256, 512, 3, 1, 1)\n",
    "        self.conv_5 = ConvBlockDiscriminator(512, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.conv_4(x)\n",
    "        x = self.conv_5(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels,\n",
    "            kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size,\n",
    "                stride, padding, padding_mode='reflect'),\n",
    "            nn.ReLU(0.2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class SimpleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size,\n",
    "                stride, padding),\n",
    "            nn.ReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Colorization(nn.Module):\n",
    "    def __init__(self, input_size=224):\n",
    "        super().__init__()\n",
    "        if input_size != 224:\n",
    "            raise ValueError(\"Input size must be 224\")\n",
    "\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        self.vgg = nn.Sequential(*list(vgg.features.children())[:-8])\n",
    "\n",
    "        self.global_features_1 = ConvBlock(512, 512, 3, 2, 1)\n",
    "        self.global_features_2 = ConvBlock(512, 512, 3, 1, 1)\n",
    "        self.global_features_3 = ConvBlock(512, 512, 3, 2, 1)\n",
    "        self.global_features_4 = ConvBlock(512, 512, 3, 1, 1)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fully_connected_1 = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Linear(512, 256),\n",
    "        )\n",
    "\n",
    "        self.fully_connected_2 = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        self.mid_level_features_1 = ConvBlock(512, 512, 3, 1, 1)\n",
    "        self.mid_level_features_2 = ConvBlock(512, 256, 3, 1, 1)\n",
    "\n",
    "        self.output_1 = SimpleConvBlock(512, 256, 1, 1, 0)\n",
    "        self.output_2 = SimpleConvBlock(256, 128, 3, 1, 1)\n",
    "        self.output_3 = SimpleConvBlock(128, 64, 3, 1, 1)\n",
    "        self.output_4 = SimpleConvBlock(64, 64, 3, 1, 1)\n",
    "        self.output_5 = SimpleConvBlock(64, 32, 3, 1, 1)\n",
    "        self.output_6 = nn.Sequential(\n",
    "            nn.Conv2d(32, 2, 3, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.up_sample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_model = self.vgg(x)\n",
    "        global_features = self.global_features_1(x_model)\n",
    "        global_features = self.global_features_2(global_features)\n",
    "        global_features = self.global_features_3(global_features)\n",
    "        global_features = self.global_features_4(global_features)\n",
    "\n",
    "        global_features2 = self.flatten(global_features)\n",
    "        global_features2 = self.fully_connected_1(global_features2)\n",
    "        global_features2 = global_features2.repeat(28, 28, 1, 1)\n",
    "        global_features2 = global_features2.permute(2, 3, 0, 1)\n",
    "\n",
    "        global_features_class = self.flatten(global_features)\n",
    "        global_features_class = self.fully_connected_2(global_features_class)\n",
    "\n",
    "        mid_level_features = self.mid_level_features_1(x_model)\n",
    "        mid_level_features = self.mid_level_features_2(mid_level_features)\n",
    "\n",
    "        fusion = torch.cat((mid_level_features, global_features2), dim=1)\n",
    "\n",
    "        output = self.output_1(fusion)\n",
    "        output = self.output_2(output)\n",
    "        output = self.up_sample(output)\n",
    "\n",
    "        output = self.output_3(output)\n",
    "        output = self.output_4(output)\n",
    "        output = self.up_sample(output)\n",
    "\n",
    "        output = self.output_5(output)\n",
    "        output = self.output_6(output)\n",
    "        output = self.up_sample(output)\n",
    "\n",
    "        return output, global_features_class\n",
    "\n",
    "def test_colorization():\n",
    "    print('Test Colorization')\n",
    "    x = torch.randn((16, 1, 224, 224))\n",
    "    x_3 = torch.cat([x, x, x], dim=1)\n",
    "    save_image(x, 'aaa.png')\n",
    "    x_3 = x_3.to(config.DEVICE)\n",
    "    colorization = Colorization(input_size=224).to(config.DEVICE)\n",
    "    colorization.apply(initialize_weights)\n",
    "    pred, _ = colorization(x_3)\n",
    "    print(type(pred))\n",
    "    print(pred[0].shape)\n",
    "    save_image(pred[0][0], '111.png')\n",
    "    save_image(pred[0][1], '222.png')\n",
    "    x = x.to(config.DEVICE)\n",
    "    pred_full = torch.cat([x, pred], dim=1)\n",
    "    save_image(pred_full, '333.png')\n",
    "\n",
    "\n",
    "def test_colorization_dataloader():\n",
    "    print('Test Colorization Dataloader')\n",
    "    colorization = Colorization(input_size=224).to(config.DEVICE)\n",
    "    test_dataloader = ColorizeDataLoader(config.TEST_PATH)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataloader, batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True, num_workers=4)\n",
    "    for idx, (gray, real, _) in enumerate(tqdm(test_dataloader)):\n",
    "        l_3 = np.tile(gray, [1, 3, 1, 1])\n",
    "        l_3 = torch.from_numpy(l_3).to(config.DEVICE)\n",
    "        colorization = Colorization(input_size=224).to(config.DEVICE)\n",
    "        colored, _ = colorization(l_3)\n",
    "        colored = colored.detach()\n",
    "        if not os.path.exists(config.OUTPUT_PATH):\n",
    "            os.makedirs(config.OUTPUT_PATH)\n",
    "        images_path = config.OUTPUT_PATH + str(idx) + '.png'\n",
    "        print('real', real.shape)\n",
    "        real_path = config.OUTPUT_PATH + str(idx) + '_real.png'\n",
    "        save_image(colored, images_path)\n",
    "        save_image(real, real_path)\n",
    "        break\n",
    "    print('Finished Data Loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_data, test_data, epochs, version=0.0):\n",
    "\n",
    "    # Load Data\n",
    "    train_dataloader = ColorizeDataLoader(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataloader, batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    test_dataloader = ColorizeDataLoader(test_data)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataloader, batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    # Load Models\n",
    "    discriminator = Discriminator(input_size=224).to(config.DEVICE)\n",
    "    discriminator.apply(initialize_weights)\n",
    "    colorization_model = Colorization(input_size=224).to(config.DEVICE)\n",
    "    vgg_model_f = models.vgg16(pretrained=True).to(config.DEVICE)\n",
    "    vgg_model_f.requires_grad_(False)\n",
    "\n",
    "    optimizer_g = Adam(\n",
    "        colorization_model.parameters(), lr=config.LR, betas=(0.5, 0.999)\n",
    "    )\n",
    "    optimizer_d = Adam(\n",
    "        discriminator.parameters(), lr=config.LR, betas=(0.5, 0.999)\n",
    "    )\n",
    "\n",
    "    # Initialize Loss\n",
    "    KKLDivergence = nn.KLDivLoss()\n",
    "    MSE = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'EPOCH {epoch} / {epochs}')\n",
    "        print('-' * 30)\n",
    "\n",
    "        for idx, (trainL, trainAB, _) in enumerate(tqdm(train_dataloader)):\n",
    "            trainL = trainL.to(config.DEVICE)\n",
    "            trainAB = trainAB.to(config.DEVICE)\n",
    "\n",
    "            l_3 = torch.cat([trainL, trainL, trainL], dim=1)\n",
    "            pred_class_vgg = F.softmax(vgg_model_f(l_3))\n",
    "            \n",
    "            # Generator Training\n",
    "            optimizer_g.zero_grad()\n",
    "            pred_AB, pred_class_c = colorization_model(l_3)\n",
    "            pred_LAB_C = torch.cat([trainL, pred_AB], dim=1)\n",
    "            with torch.no_grad():\n",
    "                dis_C = discriminator(pred_LAB_C)\n",
    "            KLD_loss = KKLDivergence(\n",
    "                F.softmax(pred_class_c).detach().float(),\n",
    "                pred_class_vgg.detach().float()\n",
    "                ) * 0.003\n",
    "            MSE_loss = MSE(pred_AB.float(), trainAB.float())\n",
    "            W_loss = wasserstein_loss(dis_C, True) * 0.1\n",
    "            g_loss = KLD_loss + MSE_loss + W_loss\n",
    "\n",
    "            # Discriminator Training\n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = True\n",
    "            optimizer_d.zero_grad()\n",
    "            pred_LAB_D = torch.cat([trainL, pred_AB], dim=1)\n",
    "            dis_pred = discriminator(pred_LAB_D)\n",
    "            dis_pred = dis_pred.mean()\n",
    "\n",
    "            true_LAB_D = torch.cat([trainL, trainAB], dim=1)\n",
    "            dis_true = discriminator(true_LAB_D)\n",
    "            dis_true = dis_true.mean()\n",
    "\n",
    "            weights = torch.randn((trainAB.size(0),1,1,1), device=config.DEVICE)\n",
    "            averaged_samples = (weights * trainAB) + ((1 - weights) * pred_AB)\n",
    "            averaged_samples = torch.autograd.Variable(averaged_samples, requires_grad=True)\n",
    "            avg_img = torch.cat([trainL, averaged_samples], dim=1)\n",
    "            dis_avg = discriminator(avg_img)\n",
    "\n",
    "            W_loss_true = wasserstein_loss(dis_true, False)\n",
    "            W_loss_pred = wasserstein_loss(dis_pred, True)\n",
    "            gp_loss_avg = partial_gp_loss(dis_avg, averaged_samples, config.GRADIENT_PENALTY_WEIGHT)\n",
    "            d_loss = W_loss_true + W_loss_pred + gp_loss_avg\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                g_loss.backward(retain_graph=True)\n",
    "                d_loss.backward()\n",
    "                optimizer_g.step()\n",
    "                optimizer_d.step()\n",
    "            \n",
    "            if config.CHECK_PER!=-1:\n",
    "                if idx % config.CHECK_PER == 0:\n",
    "                    print('\\n')\n",
    "                    print(f\"Epoch {epoch} - Batch {idx} - Loss G: {g_loss} - Loss D: {d_loss}\")\n",
    "\n",
    "    if not os.path.exists(config.MODEL_DIR):\n",
    "        os.makedirs(config.MODEL_DIR)\n",
    "    torch.save(discriminator.state_dict(), config.MODEL_D)\n",
    "    torch.save(colorization_model.state_dict(), config.MODEL_C)\n",
    "\n",
    "def train():\n",
    "    train_path = config.TRAIN_PATH\n",
    "    test_path = config.TEST_PATH\n",
    "    epochs = config.EPOCHS\n",
    "\n",
    "    print('Training Start')\n",
    "    print('-' * 30)\n",
    "\n",
    "    model(train_path, test_path, epochs)\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Training Finished')\n",
    "    print('-' * 30)\n",
    "\n",
    "    print('Testing Start')\n",
    "    print('-' * 30)\n",
    "\n",
    "    test()\n",
    "\n",
    "    print('Testing Finished')\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('All Finished')\n",
    "\n",
    "\n",
    "def sample_images(test_data, colorizationModel):\n",
    "    print('Sampling Images')\n",
    "    for idx, (gray, ori_ab, _) in enumerate(tqdm(test_data)):\n",
    "        l_3 = torch.cat([gray, gray, gray], dim=1).to(config.DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            colored, _ = colorizationModel(l_3)\n",
    "\n",
    "        gray = gray.detach().cpu().numpy()\n",
    "        ori_ab = ori_ab.detach().cpu().numpy()\n",
    "        colored = colored.detach().cpu().numpy()\n",
    "        print(\"idx\", idx)\n",
    "        for i in range(config.BATCH_SIZE):\n",
    "            original_result_red = reconstruct(deprocess(gray)[i], deprocess(colored)[i])\n",
    "            cv2.imwrite(config.OUTPUT_PATH + str(idx) + '.png', original_result_red)\n",
    "\n",
    "    print('Sampling Finished')\n",
    "\n",
    "def test():\n",
    "    path = config.MODEL_C\n",
    "    test_dataloader = ColorizeDataLoader(config.TEST_PATH)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataloader, batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    for idx, (grey_img, color_img, original_images_shape) in enumerate(test_dataloader):\n",
    "        print(f\"{idx} / {len(test_dataloader)}\")\n",
    "        print(f\"gray shape: {grey_img.shape}\")\n",
    "        print(f\"ori_ab shape: {color_img.shape}\")\n",
    "        print(f\"{original_images_shape}\")\n",
    "        break\n",
    "    colorizationModel = Colorization(input_size=224).to(config.DEVICE)\n",
    "    if config.DEVICE == 'cpu':\n",
    "        colorizationModel.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    else:\n",
    "        colorizationModel.load_state_dict(torch.load(path))\n",
    "    colorizationModel.eval()\n",
    "    sample_images(test_dataloader, colorizationModel)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.device =config.DEVICE\n",
    "\n",
    "    @staticmethod\n",
    "    def train():\n",
    "        train()\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self):\n",
    "        self.device =config.DEVICE\n",
    "\n",
    "    def test(self):\n",
    "        test()\n",
    "\n",
    "class Train:\n",
    "    def __init__(self):\n",
    "        self.trainer = Trainer()\n",
    "\n",
    "    def train(self):\n",
    "        self.trainer.train()\n",
    "        print('Training Finished')\n",
    "\n",
    "if not os.path.exists(config.OUTPUT_PATH):\n",
    "    os.makedirs(config.OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Run the following to train the model and save the model weights. Models are saved to the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train().train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Run the following to test the model and display the results. The images are saved to the `results` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester().test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
